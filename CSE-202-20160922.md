＃20160922 

- chain rule


课程大纲

1. probabilistic reasoning

   Ex: mechical diagnosis
   - knowledge respresentation: diseases cause symphoms
   - modeling uncertanty: some diseases some symphoms, more likely than others
   - Reasoning: infer diseases from symphoms
   
   
   probability: quantitantine, self-consistent framework that captures commonsense palforms of reasoning

   How do graphs to represent corelation, causation, independence?
   ---> marriage of probability & graph theory
   
2. Prediction

  	Ex: span filter
  	
    Input: email message
    
    Output: {spam, not spam}
   
   	
   	How to represent input? (pic)
   	Convert text to fixed-length rector of word counts:
   	Ci = times that ith word in dictionary appears in email
   	
   	V = vocabulary size, # entries in dictionary
   	
   	* graphical model(pic) 
   	
   	Ex: handwriting recognition
   	inputs: grayscale images eg.20x20 image
   	outputs:labels 0,1,2,...,9
   	represent each image by vector x属于R^400 with one element per pixel
   	
   	* more generally classification
   	input:{x1,x2,..xN} xi属于R^D（d-dimeansion）
   	output:[y1,y2,...yN], yi属于{1,2,...C},C是＃classes
   	
   	
   	
3. Pattern analysis & discovery
   	
Ex: topic modeling

how to organize large collection of documents by "topic"  <-- content
    
* more generally, clustering
   	
  inputs {x1, x2, ..., xN}, xi属于R^D
  
  how to group inputs when no labels are provided?
  D=2 demensions
  
4. sequential modeling

* how to model systems whose "stale" changes over time(or has a similarly extended representation)?

EX: text(written language)
"states" =  words

which sentence is more likely?
1) May had a little lamb.
2) Colorless green ideas sleep funously.
=> Markow models for statistied language processing.

Graphical models
   	(pic)

Model A: richer but harder to estimate
Model B: wrong but easier to estimate

__

EX: speech (spoken language)
states = words (or syllabels, characters)
observations = sounds, wareforms
   	pic:声波图
   	
How do we infer words from waveforms?
---> hidden Mavkov models
   	
   	
5. Planning and decision-making
EX: robot navigation   	
   	2d grid world
"states" =  2D grid cell   	(pic)
"actors" = attempt to move N,S,E,W

* noisy dynamics
* rewards = feedback signals from environment

-immediate VS delayed
-instructive VS evaluative

More Generally:
How can autonomous agents learn from experience?
-->Markev Decision process

Other embeded agents: self-driving car
Embedded: enemy AI, computer telephones operator

   	
   	
   	
   	
   	
   	
   	